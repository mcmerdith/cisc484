{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b434fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#VGG\n",
    "###############################################################################\n",
    "\n",
    "#It specifies what (convolution, batch normalization, ReLU activation, dropout) are needed to build the network.\n",
    "\n",
    "#Dropout is a regularization technique commonly used in neural networks to prevent overfitting.\n",
    "#It works by randomly setting a fraction of input units to zero during the training process, which helps to prevent the model from relying too heavily on specific input features and encourages the network to learn more robust and generalizable representations.\n",
    "\n",
    "#this class encapsulates the operations of a single convolutional block, including convolution, batch normalization, ReLU activation, and optionally dropout.\n",
    "\n",
    "#stride: step size\n",
    "#padding: ensures that the output feature map has the same spatial dimensions as the input.\n",
    "\n",
    "class vgg16_conv_block(nn.Module):\n",
    "    def __init__(self, input_channels, out_channels, rate=0.3, drop=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, out_channels, 3, 1, 1)  #This line creates a 2D convolutional layer using PyTorch's nn.Conv2d module.\n",
    "        self.bn = nn.BatchNorm2d(out_channels) # normalize the activations of the convolutional layer in the neural network.\n",
    "        self.relu = nn.ReLU(inplace=True) #introduces non-linearity (-ve --> zeros)\n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.drop = drop\n",
    "\n",
    "    def forward(self, x):  #This method defines the forward pass of the convolutional block.\n",
    "        x = self.relu(self.bn(self.conv(x)))\n",
    "        if self.drop:\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# It specifies how many (convolutional blocks) to build and what features each network should have\n",
    "# creating a sequence of convolutional blocks followed by a max-pooling layer\n",
    "# 0.3 corresponds to the dropout rate for the convolutional blocks within each layer, and the second value 0.4 represents the dropout rate for the fully connected layers in the VGG16 model.\n",
    "\n",
    "def vgg16_layer(input_channels, out_channels, num, dropout=[0.3, 0.4]):\n",
    "    layers = []\n",
    "    for i in range(num):  #creating convolutional blocks using the vgg16_conv_block function.\n",
    "        layers.append(vgg16_conv_block(input_channels, out_channels, dropout[0]))\n",
    "        input_channels = out_channels  #ensuring that subsequent blocks receive the correct number of input channels.\n",
    "    layers.append(nn.MaxPool2d(2, 2))  #reduces the spatial dimensions of the feature maps by a factor of 2\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# This is like the blueprint for the entire neural network.\n",
    "# It specifies how to arrange the (layers) and what features each layers should have.\n",
    "\n",
    "class ModifiedVGG16(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModifiedVGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            vgg16_layer(1, 16, 2),  # Adjust input channels to 1\n",
    "            vgg16_layer(16, 32, 2),\n",
    "            vgg16_layer(32, 64, 2),\n",
    "            vgg16_layer(64, 128, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 10, bias=True)\n",
    "        )\n",
    "\n",
    "    # defines how data flows through the network during the forward pass.\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078f79fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for the dataset\n",
    "\n",
    "# ***** Add more data augmenation\n",
    "#   random rotation, horizontal flip, random affine transformation,\n",
    "#   random vertical flip, random perspective transformation, random brightness adjustment,\n",
    "#   and random contrast adjustment   **********\n",
    "\n",
    "transform_plus = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load the full MNIST dataset\n",
    "trainset_full = torchvision.datasets.MNIST('./data', train=True, download=True, transform=transform_plus)\n",
    "testset_full = torchvision.datasets.MNIST('./data', train=False, download=True, transform=transform_plus)\n",
    "\n",
    "# Define the number of samples you want to use for training and testing\n",
    "num_train_samples = 5000  # Example: Use 5000 samples for training\n",
    "num_test_samples = 1000   # Example: Use 1000 samples for testing\n",
    "\n",
    "# Create subsets of the full dataset\n",
    "train_indices = torch.randperm(len(trainset_full))[:num_train_samples]\n",
    "test_indices = torch.randperm(len(testset_full))[:num_test_samples]\n",
    "\n",
    "trainset= Subset(trainset_full, train_indices)\n",
    "testset= Subset(testset_full, test_indices)\n",
    "\n",
    "# Load a batch of images and labels from the trainset\n",
    "batch_size = 15\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Define a function to display a grid of images\n",
    "def show_images(images, labels, nrows=3, ncols=5, figsize=(10, 6)):\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "        ax.set_title(f\"Label: {labels[i]}\")\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Iterate over the trainloader to get a batch of data\n",
    "for images, labels in trainloader:\n",
    "    # Display the images\n",
    "    show_images(images, labels)\n",
    "    break  # Break after displaying the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccca801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classes\n",
    "classes = tuple(str(i) for i in range(10))  # MNIST has 10 classes, digits 0 to 9\n",
    "\n",
    "net = ModifiedVGG16()\n",
    "net = nn.DataParallel(net)\n",
    "net.to(device)\n",
    "net.train()\n",
    "\n",
    "\n",
    "#      ***students can modify the training hyperparameters\n",
    "epoch_num=4 # 20  #the number of times the entire dataset will be fed forward and backward through the network during training.\n",
    "batch_num=256 #the number of training examples used in each iteration of training.\n",
    "learning_rate=0.05  #Determines the step size at which the model parameters are updated during training using the gradient descent algorithm.\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "# This function test is used to evaluate the performance of a neural network model (net) on a given dataset (dataloader).\n",
    "def test(net, dataloader):\n",
    "    test_loss=0\n",
    "    test_correct=0\n",
    "    time=0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        idx=0\n",
    "        for data in dataloader:\n",
    "            inputs, labels =data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs=net(inputs)\n",
    "            test_loss += loss_fn(outputs, labels).item()*len(labels)\n",
    "            #Compare the predicted class (index of the maximum value in the output tensor) with the actual labels to determine the number of correct predictions. Sum up the correct predictions across all the batches.\n",
    "            test_correct += (torch.max(outputs.data,1)[1] == labels).sum()\n",
    "            time += 1\n",
    "\n",
    "    return(test_loss/len(testset), test_correct/len(testset)*100)\n",
    "\n",
    "#   ***** use two more optimizer, two different learning rates and see the results ******\n",
    "opt=optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-6, nesterov=True) #Specifies the optimizer used for updating the model parameters based on the computed gradients\n",
    "\n",
    "train_num=len(trainset)//batch_num\n",
    "\n",
    "#These lists will store the loss and accuracy values during training and testing.\n",
    "los=[]\n",
    "cor=[]\n",
    "train_los=[]\n",
    "train_cor=[]\n",
    "test_los=[]\n",
    "test_cor=[]\n",
    "\n",
    "#These variables store the best model's performance metrics, such as accuracy, loss, learning rate, and epoch number.\n",
    "net_corr, net_los,net_train_los, net_train_corr, net_lr, net_epoch = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "#These tensors are initialized to store the training and testing accuracies for each epoch.\n",
    "train_cor = torch.zeros(epoch_num)  # Initialize train_cor tensor\n",
    "test_cor = torch.zeros(epoch_num)   # Initialize test_cor tensor\n",
    "\n",
    "#Creates a data loader for the training and testing dataset (trainset, testset) to load batches of training data.\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_num, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100)\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    loss_avg=0\n",
    "    train_time=0\n",
    "    correct=0\n",
    "    num_img=0\n",
    "\n",
    "    # Iterates over batches of data from the trainloader, which contains batches of training images and their corresponding labels\n",
    "\n",
    "    for data in trainloader:  #iterates over batches of data from the trainloader, which contains batches of training images and their corresponding labels\n",
    "\n",
    "        # Separates inputs (images) and labels from the current batch\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Moves inputs and labels to the specified device (e.g., GPU)\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        net.train() # Sets the model to training mode\n",
    "        outputs=net(inputs)    # Performs forward pass: computes outputs (predictions) of the model for the input images\n",
    "\n",
    "        loss=loss_fn(outputs, labels)     # Computes the loss between the predicted outputs and the ground truth labels\n",
    "\n",
    "        loss.to(device)     # Moves the loss tensor to the specified device\n",
    "\n",
    "        opt.zero_grad()     # Clears the gradients of all optimized tensors\n",
    "\n",
    "        loss.backward()     # Backpropagates the gradients: computes gradients of the loss with respect to model parameters\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), 20)      # Clips gradients to prevent exploding gradients problem\n",
    "\n",
    "        opt.step()     # Updates the model parameters (weights) based on the computed gradients and the optimizer's update rule\n",
    "\n",
    "        # Updates training statistics\n",
    "\n",
    "        train_time+=1\n",
    "        loss_avg += loss.item()*len(labels)\n",
    "        predict = torch.max(outputs.data,1)[1]\n",
    "        correct += (predict == labels).sum()\n",
    "        num_img += len(labels)\n",
    "        sys.stdout.flush()\n",
    "    print('\\r', end=\"\")\n",
    "\n",
    "    # Evaluates the model on the test dataset and retrieves the test loss and accuracy\n",
    "\n",
    "    los2,cor2 = test(net, testloader)\n",
    "    print('Training: {}/{} epoch, Learning rate: {:.10f}, Average Loss:{:.2f}, Accuracy: {:.2f}%, Test Loss:{:.2f}, Test accuracy:{:.2f}%'\n",
    "          .format(epoch+1,epoch_num, opt.state_dict()['param_groups'][0]['lr'], loss_avg/num_img, correct/num_img*100,los2, cor2.item()))\n",
    "\n",
    "    los.append(los2)\n",
    "    cor.append(cor2)\n",
    "    train_cor[epoch] = correct/num_img*100  # Update train_cor tensor\n",
    "    test_cor[epoch] = cor2.item()           # Update test_cor tensor\n",
    "    train_los.append(loss_avg/num_img)\n",
    "    test_los.append(los2)\n",
    "    if net_corr < cor2: #This condition checks if the current test accuracy (cor2) is greater than the previously recorded best test accuracy (net_corr).\n",
    "        net_corr, net_los,net_train_los, net_train_corr, net_lr, net_epoch = cor2, los2, loss_avg/num_img,correct/num_img,opt.state_dict()['param_groups'][0]['lr'], epoch+1\n",
    "    torch.save(net, './net_model.pkl')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "print('The {} epoch achieves the best model, Test Loss: {:.4f}, Test accuracy: {:.2f}%'.format(net_epoch, net_los, net_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3939eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ****** Plotting the training and test loss and accuracy over epochs *******\n",
    "\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.plot(range(epoch_num), los)\n",
    "plt.xticks(range(epoch_num))\n",
    "plt.show()\n",
    "\n",
    "# Plotting the training accuracy\n",
    "plt.plot(range(epoch_num), cor)\n",
    "plt.xticks(range(epoch_num))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92688c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary\n",
    "from torchinfo import summary\n",
    "\n",
    "# Define a function to get the input size from the first batch of data\n",
    "def get_input_size(dataloader):\n",
    "    # Get the first batch of data\n",
    "    inputs, _ = next(iter(dataloader))\n",
    "    # Return the input size\n",
    "    return inputs.size()[1:]\n",
    "\n",
    "# Get the input size for MNIST images\n",
    "input_size = get_input_size(trainloader)\n",
    "\n",
    "# Print the input size\n",
    "print(\"Input size for MNIST images:\", input_size)\n",
    "\n",
    "# Create an instance of the ModifiedVGG16 model\n",
    "net = ModifiedVGG16().to(device)\n",
    "\n",
    "# Print the summary\n",
    "summary(net, input_size=(batch_size, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7045f325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f41414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_train = MNIST(root=\"./data\", train=True, download=True)\n",
    "\n",
    "# Select a random image from the dataset\n",
    "index = np.random.randint(0, len(mnist_train))\n",
    "image, label = mnist_train[index]\n",
    "\n",
    "# Define transformations\n",
    "transformations = [\n",
    "    transforms.RandomRotation(15),               # Random rotation by up to 15 degrees\n",
    "    transforms.RandomVerticalFlip(60),            # Random vertical flip\n",
    "]\n",
    "\n",
    "# Plot original image\n",
    "plt.figure(figsize=(14, 10))\n",
    "plt.subplot(3, 4, 1)\n",
    "plt.title(\"Original\")\n",
    "plt.imshow(np.array(image), cmap='gray')\n",
    "plt.axis('off')\n",
    "\n",
    "# Apply transformations and plot augmented images\n",
    "for i, transform in enumerate(transformations):\n",
    "    augmented_image = transform(image)\n",
    "    plt.subplot(3, 4, i + 2)\n",
    "    plt.title(f\"Augmented {i+1}\")\n",
    "    plt.imshow(np.array(augmented_image), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cisc484",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
