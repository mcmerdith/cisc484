# Homework 5

All the optimizers I tested were very similar. ADAM and LION were basically the same. Adafactor converged faster but was slightly worse.

## Initial Model

![Initial Model](adam_numEpochs_10-learningRate_0.001.png)

## LION Optimizer

![LION](lion_numEpochs_10-learningRate_0.001.png)

## Adafactor Optimizer

![Adafactor](adafactor_numEpochs_10-learningRate_0.001.png)

