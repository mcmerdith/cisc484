# Introduction

My model performed similarly to the scikit-learn models.

Reports generated by each run are available in the `Results` folder.

To keep this report concise, the numerical results are not included in this document. The final performance report is available in the [report_1731911640.290229_final.md](Results/report_1731911640.290229_final.md) file

I left some of the intermediate reports as well if you're interested to see the changes over time as I corrected issues with the models design. However, all reports are from after I considered the model "stable" and only include the results of smaller tweaks to parameters and design.

## Included Features

The features generated by `gen_feature.py` are available in the `Tmp` folder. They are the same reduced feature set used in the 2nd homework assignment. In my testing adding additional features did not improve performance and in some cases harmed it. The `gen_feature.py` and `Twitter.py` scripts are included as well for completeness.

### Other files

This branch is a fork of the original `hw2` branch so it includes all of the reports from the original assignment, however I have renamed them to differentiate them from the new reports.

# Model Performance

The models performed very similarly to the scikit-learn models. They performed better in some ways but worse in others.

For the random forest mode, I saw improvements in performance by increasing the bootstrap size, but did not push it past 75% of the dataset as the training time was getting out of hand.

## Comparison between the `nearest` and `majority` methods for untrained data

Initially, `nearest` had better performance, however as I refined the model, `majority` began to outperform it. In the current state of the model, `nearest` has lower accuracy by about 10%, and its increased identification of positives comes at the cost of a higher number of false positives.

Based on these results, I will be using the `majority` method for comparison to the scikit-learn models.

My implementation of the random forest lags behind the scikit-learn model in all metrics, but still has respectable performance. This is most likely due to more advanced optimization and better handling of multiple decision trees by the scikit models.

However, in the case of the data that is used for this project, my decision tree actually outperformed the scikit model in all metrics except recall. This is likely due to my model's handling of unknown values, since the `majority` method is more likely to return a "negative" label for the dataset we are using.

## Computational Efficiency

My models are not particularly computationally efficient. Especially when using the random forest with higher bootstrap sizes, the model takes significantly longer to train. I solved this by throwing more processors at the problem, which while short sighted is sufficient for this project. The extensive training time can be mostly attributed to the amount of recursion being done as well as the repeated recomputing of arrays during tree creation. I did not put much, if any, effort into caching or other optimizations.

# Procedure and Model Concepts

I implemented a fairly simplistic decision tree model. The random forest is built on top of the decision tree, using bagging to train the trees.

## Concepts Implemented in the Decision Tree

I implemented 3 types of nodes:

1. Branch Nodes
    - The default node type. Contains `n`-branches and attempts a direct match
    - Branch nodes will fall back to either the nearest value or the majority label if a direct match is not found.
    - Branch nodes eliminate the feature they split on from being split again.
2. Binary Nodes
    - Uses a threshold to split the data into two sets along a threshold value which gives the highest information gain.
    - Binary nodes do not eliminate the feature they split on from being split again.
    - Notably, binary nodes will fall back to branch nodes if no suitable threshold is found.
3. Leaf Nodes
    - Contains just a label.
    - In my implementation, leaf nodes are always "pure" and are only created when splitting is no longer possible.

The decision tree is built recursively by selecting the feature with the highest information gain and splitting the data by that feature into either a binary node or branch node depending on the number of unique values.

## Implemented Optimizations

To alleviate over-fitting, I implemented the binary split nodes to provide more general classification during the earlier parts of training when only branch nodes would be susceptible to shattering due to the real-valued nature of the data.

I did not implement any pruning, depth-limiting, statistical significance testing, boosting, or other optimizations. I also did not implement any metrics for information gain, however these would likely not pose significant difficulty to add.

## Issues Encountered

Initially, branch nodes failed to predict values due to values seen in testing that it was not trained on. I solved this using 2 different methods: `nearest` and `majority`. The `nearest` method finds the feature value closest to the unknown value and treats the unknown value as if it was that value. The `majority` method returns the most common label.

The next issue came with the the introduction of binary nodes. If the binary node wasn't able to find a threshold to split on, the model would fail to product an output. I solved this by adding a fallback to branch nodes if no suitable threshold was found.

I'm not sure these solutions are the most optimal or correct, but in my testing they seemed to be sufficient.

# Reports

See reports generated by each run in the `Results` folder.