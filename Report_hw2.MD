# Model Performance

The models did not perform particularly well.

See reports generated by each run in the `Results_hw2` folder.

Each run was generated using the same data, but with different feature sets.

# Notes compared to original submission

Altering the feature set marginally improved the performance of the models.
However I was not able to find a set of features that resulted in what could be considered good performance.

# Analysis of model performance

The performance of the models is not particularly surprising.

SVM and logistic regression struggled to correctly identify the majority of positives.
I suspect this is due to the particularly varied nature of spam tweets.

The tree-based models identified more positives, but at the cost of a higher number of false positives.

Random forest likely benefited from the more random nature of its training data selection over decision tree.

## SVM compared to other models

SVM and logistic regression performed identically.
They had decent accuracy, but failed to identify the majority of positives.
However, when a positive was identified it was always correct.

Random Forest and Decision Tree identified more positives but with lower precision.
Decision Tree identified the most positives but at the lowest precision.

Random Forest seems to perform best overall but did not have particularly high correct positive identification.
However, it had the best identification of the models that actually identified positives.
